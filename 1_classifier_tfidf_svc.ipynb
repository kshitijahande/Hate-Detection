{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-classifier-tfidf-svc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNbwuGfYi+67Lix6jVpQBZd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kshitijahande/Hate-Detection/blob/main/1_classifier_tfidf_svc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdAhZNPCeEm4"
      },
      "source": [
        "!pip install -qq ipynb\n",
        "!pip install -qq plotly\n",
        "!pip install -qq import-ipynb\n",
        "!pip install -qq icecream\n",
        "!pip install -qq transformers\n",
        "\n",
        "!pip install -qq datasets\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NAfJkQTeKo3"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random as rn\n",
        "from icecream import ic \n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoIVQmfPd_T9",
        "outputId": "ff6fe222-bb5e-4261-9731-e785fcffcaf3"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# ri = (datasets.ReadInstruction('train', to=10, unit='%') +\n",
        "# datasets.ReadInstruction('train', from_=-80, unit='%'))\n",
        "# dataset = datasets.load_dataset('hate_speech_offensive', split=ri)\n",
        "dataset = load_dataset('hate_speech_offensive', split='train')\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset hate_speech_offensive (/root/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlnZda7Yep2T",
        "outputId": "b5cc14ec-286b-484d-a50f-1ee7b30f7f02"
      },
      "source": [
        "#print dataset properties\n",
        "ic(dataset[1]['tweet'])\n",
        "ic(dataset[1])\n",
        "ic(dataset.column_names)\n",
        "ic(dataset.num_rows)\n",
        "\n",
        "# Task 1 - binary classification of hate=class:0 vs non-hate=class:1,2 \n",
        "# \"class\":[\n",
        "# 0:\"hate speech\"\n",
        "# 1:\"offensive language\"\n",
        "# 2:\"neither\"\n",
        "# ]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ic| dataset[1]['tweet']: ('!!!!! RT @mleew17: boy dats cold...tyga dwn bad for cuffin dat hoe in the '\n",
            "                          '1st place!!')\n",
            "ic| dataset[1]: {'class': 1,\n",
            "                 'count': 3,\n",
            "                 'hate_speech_count': 0,\n",
            "                 'neither_count': 0,\n",
            "                 'offensive_language_count': 3,\n",
            "                 'tweet': '!!!!! RT @mleew17: boy dats cold...tyga dwn bad for cuffin dat hoe '\n",
            "                          'in the 1st place!!'}\n",
            "ic| dataset.column_names: ['count',\n",
            "                           'hate_speech_count',\n",
            "                           'offensive_language_count',\n",
            "                           'neither_count',\n",
            "                           'class',\n",
            "                           'tweet']\n",
            "ic| dataset.num_rows: 24783\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24783"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PFqOQY2JM32",
        "outputId": "541629ea-9905-4989-957a-bdbc27a70d41"
      },
      "source": [
        "#shuffle and split train and test dataset randomly\n",
        "train_test_split = dataset.train_test_split(shuffle=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5/cache-d21fa33e190556a1.arrow and /root/.cache/huggingface/datasets/hate_speech_offensive/default/1.0.0/5f5dfc7b42b5c650fe30a8c49df90b7dbb9c7a4b3fe43ae2e66fabfea35113f5/cache-0b5f0974b266240e.arrow\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVc7Kw-tJmkP"
      },
      "source": [
        "#get train and set data from original dataset\n",
        "test = train_test_split.get('test')\n",
        "train = train_test_split.get('train')\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82ded0TgRkcB"
      },
      "source": [
        "#convert to data frame\n",
        "test_df = test.to_pandas()\n",
        "train_full_df = train.to_pandas()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL6fifivZKk3"
      },
      "source": [
        "# split train dev/validation set\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(train_full_df, test_size=0.2)\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUsFUSoDVDUr",
        "outputId": "791d0462-efa8-451c-f0b9-21ca2bee9d7d"
      },
      "source": [
        "# match total rows in each data frame\n",
        "ic(train_df.shape[0])\n",
        "ic(test_df.shape[0])\n",
        "ic(dev_df.shape[0])\n",
        "total = train_df.shape[0] + test_df.shape[0] + dev_df.shape[0]\n",
        "ic(total)\n",
        "ic(dataset.num_rows)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ic| train_df.shape[0]: 14869\n",
            "ic| test_df.shape[0]: 6196\n",
            "ic| dev_df.shape[0]: 3718\n",
            "ic| total: 24783\n",
            "ic| dataset.num_rows: 24783\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24783"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_AiK4XKfRKh",
        "outputId": "02e00755-53dd-4a07-9f3f-82b176898532"
      },
      "source": [
        "# hate vs non-hate distribution in data\n",
        "train_df['class'].value_counts(ascending=True)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    11475\n",
              "2     2533\n",
              "0      861\n",
              "Name: class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KJSelosp0Ae"
      },
      "source": [
        "FEATURE GENERATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsKienV9p3kO",
        "outputId": "f63b9c48-8340-4863-f669-fd5649ddb8d1"
      },
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "import string\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dPNT2BWlNFL"
      },
      "source": [
        "# Use tf-idf vectorizer\n",
        "\n",
        "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
        "stopwords=stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "stopwords.extend(other_exclusions)\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess(text_string):\n",
        "    \"\"\"\n",
        "    replaces:\n",
        "    1) urls with URLHERE\n",
        "    2) lots of whitespace with one instance\n",
        "    3) mentions with MENTIONHERE\n",
        "    standardized counts\n",
        "    \"\"\"\n",
        "    space_pattern = '\\s+'\n",
        "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
        "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "    mention_regex = '@[\\w\\-]+'\n",
        "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
        "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
        "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
        "    return parsed_text\n",
        "\n",
        "def tokenize(tweet):\n",
        "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
        "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
        "    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
        "    #tokens = re.split(\"[^a-zA-Z]*\", tweet.lower())\n",
        "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
        "    return tokens\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
        "    tokenizer=tokenize,\n",
        "    preprocessor=preprocess,\n",
        "    ngram_range=(1, 3),\n",
        "    stop_words=stopwords, #We do better when we keep stopwords\n",
        "    use_idf=True,\n",
        "    smooth_idf=False,\n",
        "    norm=None, #Applies l2 norm smoothing\n",
        "    decode_error='replace',\n",
        "    max_features=10000,\n",
        "    min_df=5,\n",
        "    max_df=0.501\n",
        "    )"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDbNkaTzsnyl",
        "outputId": "b70e49f9-cfee-442d-9ce1-36eb97972620"
      },
      "source": [
        "train_df['tweet']"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3776     RT @ahmadblack35: Can someone find the Georgia...\n",
              "14042                             Where's my money, bitch?\n",
              "13366                   @ReiddH i agree my fellow cracker.\n",
              "1171     What you gonna do if she cheat ? Cry ? Fight f...\n",
              "9215     &#8220;@__thaRealist: &#128530; dis hoe BRI wi...\n",
              "                               ...                        \n",
              "5356     @TooEffinCool lol I feel like a punk bitch sit...\n",
              "3078                            These bitches crazzzyyyyyy\n",
              "17987                       @SammyLightning_ Tim's a pussy\n",
              "10388    RT @AzlanTheGoat: \"These hoe ain't loyal\"\\n\\n-...\n",
              "3968     I'm still weak off the bitch gettin pissed on lol\n",
              "Name: tweet, Length: 14869, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eK87LXwXsUN9",
        "outputId": "c54f0657-7897-45b9-edab-a63f3279c2e1"
      },
      "source": [
        "\n",
        "# tfidf matrix and scores\n",
        "tfidf = vectorizer.fit_transform(train_df['tweet']).toarray()\n",
        "vocab = {v:i for i, v in enumerate(vectorizer.get_feature_names())}\n",
        "idf_vals = vectorizer.idf_\n",
        "idf_dict = {i:idf_vals[i] for i in vocab.values()} #keys:indices; values:IDF scores"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'e', 'f', 'g', 'h', 'j', 'l', 'n', 'p', 'r', 'u', 'v', 'w'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcgZDaBBtNAv",
        "outputId": "a103066b-1b20-4b29-d9db-d162e3543abb"
      },
      "source": [
        "tfidf.shape"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14869, 3028)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X9goUyOtgE6"
      },
      "source": [
        "TASK 1 - TRAIN BINARY CLASSIFIER FOR HATE VS NON-HATE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cmgaAjttmbP"
      },
      "source": [
        "X = pd.DataFrame(tfidf)\n",
        "y = train_df['class'].astype(int)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBusvsBOt2PU"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import LinearSVC"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zgmx5gQGt6EW",
        "outputId": "81000fd8-ca24-4af4-ce0e-1bab8d5e8a66"
      },
      "source": [
        "select = SelectFromModel(LogisticRegression(class_weight='balanced',penalty=\"l2\",C=0.01))\n",
        "X_ = select.fit_transform(X,y)\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zxm47gubuzOe"
      },
      "source": [
        "model = LinearSVC(class_weight='balanced',C=0.01, penalty='l2', loss='squared_hinge',multi_class='ovr').fit(X_, y)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX3HqMUhu5bM"
      },
      "source": [
        "y_preds = model.predict(X_)\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofUh0R5Ku9A-"
      },
      "source": [
        "report = classification_report( y, y_preds )\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_BQ12dhvAjt",
        "outputId": "ae4ac2b2-d349-4e3c-a930-83301d61f4ff"
      },
      "source": [
        "print(report)\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.76      0.65       861\n",
            "           1       0.96      0.91      0.93     11475\n",
            "           2       0.78      0.90      0.84      2533\n",
            "\n",
            "    accuracy                           0.90     14869\n",
            "   macro avg       0.77      0.86      0.81     14869\n",
            "weighted avg       0.91      0.90      0.90     14869\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}